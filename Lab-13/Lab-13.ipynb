{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZKrlwajuGZ-"
   },
   "source": [
    "# Lab-13: Ensemble Learning\n",
    "\n",
    "In this lab, we will look at different ways to build ensemble models.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "* Bagging\n",
    "* Random Forests\n",
    "* AdaBoost\n",
    "\n",
    "\n",
    "Why ensemble learning? How does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G2NXb2yznzd"
   },
   "source": [
    "## Ensemble learning\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9sI82NDtzoSP",
    "outputId": "a3162ec7-b6cb-4258-dcc6-86eca1157e99"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcJSouftKwUC"
   },
   "source": [
    "### Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aqrQbHVDKw9F",
    "outputId": "c7ebcbc8-41cd-49c3-8d44-5bed82c74265"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgAj4l5GLKuG"
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How? \n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "xWQTCYrGLLMM",
    "outputId": "c762f1dd-631c-40d3-f1cd-d45855890388"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    pass\n",
    "\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = None\n",
    "\n",
    "print(base_pred)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODOPvJJ2bKPh"
   },
   "source": [
    "Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all?\n",
    "\n",
    "\n",
    "### Random forest\n",
    "\n",
    "Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n",
    "\n",
    "At first, let's implement bootstrap sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "C9d1ElFpE48c",
    "outputId": "9c6fc189-e7fa-4b2a-a5db-41403aab23c4"
   },
   "outputs": [],
   "source": [
    "def bootstrap(X, y):\n",
    "    # generate bootstrap indices and return data according to them\n",
    "    X_bs = None\n",
    "    y_bs = None\n",
    "    return X_bs, y_bs\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxpCck94Y73A"
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "       \n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HxIhmI_H5jnI",
    "outputId": "b2e3e694-f1a1-4513-b281-f7acac8febe1"
   },
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    pass\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObGBbQfrE5jM"
   },
   "source": [
    "And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp4WWdYezpHy"
   },
   "source": [
    "## Boosting\n",
    "\n",
    "How does boosting work? \n",
    "\n",
    "For simplicity let's make a binary classification problem out of the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JBbO7p-aNk69"
   },
   "outputs": [],
   "source": [
    "y_train_b = (y_train == 2 ) * 2 - 1\n",
    "y_test_b = (y_test == 2 ) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "talvfivTQitE"
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mVEBRi4pEwte"
   },
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "# initialize sample weights\n",
    "sample_weights = None\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3)\n",
    "    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n",
    "    # caclulate tree weight\n",
    "    w = None\n",
    "    tree_weights[i] = None\n",
    "    classifiers.append(clf)\n",
    "    # update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y[j]:\n",
    "            # in case of a misclassification\n",
    "            pass\n",
    "        else:\n",
    "            # in case when the prediction is correct\n",
    "            pass\n",
    "    # normalize the weights\n",
    "    sample_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ43LIorSXbs"
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PpJKjCDdzpmt",
    "outputId": "4d7db356-1a10-413f-c432-1877b3f46b72"
   },
   "outputs": [],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "# caclulate predictions\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_b, pred)\n",
    "print(\"Boosting accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7mv1TfwSahW"
   },
   "source": [
    "The resulting accuracy is **0.97**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
