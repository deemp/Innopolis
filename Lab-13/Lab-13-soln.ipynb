{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZKrlwajuGZ-"
   },
   "source": [
    "# Lab-13: Ensemble Learning\n",
    "\n",
    "In this lab, we will look at different ways to build ensemble models.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "* Bagging\n",
    "* Random Forests\n",
    "* AdaBoost\n",
    "\n",
    "\n",
    "Why ensemble learning? How does it help? <span style=\"color:blue\"> By combining the power of multiple models in a single model while overcoming their weaknesses, thus reducing variance and/or bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G2NXb2yznzd"
   },
   "source": [
    "## Ensemble learning\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "9sI82NDtzoSP",
    "outputId": "a3162ec7-b6cb-4258-dcc6-86eca1157e99"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcJSouftKwUC"
   },
   "source": [
    "### Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aqrQbHVDKw9F",
    "outputId": "c7ebcbc8-41cd-49c3-8d44-5bed82c74265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single tree accuracy: 0.8619528619528619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgAj4l5GLKuG"
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How? <span style=\"color:blue\"> Averaging reduces variance.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "xWQTCYrGLLMM",
    "outputId": "c762f1dd-631c-40d3-f1cd-d45855890388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3 ... 3 8 8]\n",
      " [8 8 8 ... 8 8 8]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "Bagging accuracy: 0.8787878787878788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "print(base_pred)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODOPvJJ2bKPh"
   },
   "source": [
    "Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all? <span style=\"color:blue\"> That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random.\n",
    "\n",
    "\n",
    "### Random forest\n",
    "\n",
    "Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n",
    "\n",
    "At first, let's implement bootstrap sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "C9d1ElFpE48c",
    "outputId": "9c6fc189-e7fa-4b2a-a5db-41403aab23c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [ 3,  4,  5],\n",
       "        [ 0,  1,  2]]),\n",
       " array([0, 3, 1, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y):\n",
    "    # generate bootstrap indices and return data according to them\n",
    "    ind = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "    return X[ind,:], y[ind]\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxpCck94Y73A"
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "       \n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HxIhmI_H5jnI",
    "outputId": "b2e3e694-f1a1-4513-b281-f7acac8febe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    base = DecisionTreeClassifier(max_features=\"sqrt\")\n",
    "    bs_X, bs_y = bootstrap(X_train, y_train)\n",
    "    base.fit(bs_X, bs_y)\n",
    "    classifiers.append(base)\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObGBbQfrE5jM"
   },
   "source": [
    "And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp4WWdYezpHy"
   },
   "source": [
    "## Boosting\n",
    "\n",
    "How does boosting work? <span style=\"color:blue\"> Models are built sequentially: each model is built using information from previously built models. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "\n",
    "For simplicity let's make a binary classification problem out of the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JBbO7p-aNk69"
   },
   "outputs": [],
   "source": [
    "y_train_b = (y_train == 2 ) * 2 - 1\n",
    "y_test_b = (y_test == 2 ) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "talvfivTQitE"
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mVEBRi4pEwte"
   },
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "# initialize sample weights\n",
    "sample_weights = np.ones(train_samples) / train_samples\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3)\n",
    "    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n",
    "    # caclulate tree weight\n",
    "    w = 0.5 * np.log(acc / (1 - acc))\n",
    "    tree_weights[i] = w\n",
    "    classifiers.append(clf)\n",
    "    # update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y[j]:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp(w)\n",
    "        else:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp((-w))\n",
    "    # normalize the weights\n",
    "    sample_weights = sample_weights / np.sum(sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ43LIorSXbs"
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PpJKjCDdzpmt",
    "outputId": "4d7db356-1a10-413f-c432-1877b3f46b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting accuracy: 0.9730639730639731\n"
     ]
    }
   ],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "# caclulate predictions\n",
    "for t in range(n_trees):\n",
    "    pred += classifiers[t].predict(X_test) * tree_weights[t]\n",
    "for i in range(n_test):\n",
    "    pred[i] = 1 if pred[i] > 0 else -1\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_b, pred)\n",
    "print(\"Boosting accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7mv1TfwSahW"
   },
   "source": [
    "The resulting accuracy is **0.97**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
