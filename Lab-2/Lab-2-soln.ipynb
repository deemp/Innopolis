{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-2: Linear Regression.\n",
    "_____\n",
    "##### In this lab, you will Practice applying different Regression algorithms, and play with models to experiece underfitting vs. overfitting.\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "1. Simple Linear Regression\n",
    "2. Multiple Linear Regression\n",
    "3. Polynomial Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "1. What is linear regression?\n",
    "<span style=\"color:blue\">Regression searches for relationships among variables. Generally, in regression analysis, we usually consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that (at least) one of the features depends on the others, we try to establish a relation among them. The dependent features are called the dependent variables, outputs, or responses. The independent features are called the independent variables, inputs, or predictors.</span>\n",
    "2. When Do We Need Regression?\n",
    "<span style=\"color:blue\"> Typically, we need regression to answer whether and how some phenomenon influences the other or how several variables are related. For example, we can use it to determine if and to what extent the experience or gender impact salaries. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "1. What is linear regression?\n",
    "<span style=\"color:blue\">When implementing linear regression of some dependent variable ùë¶ on the set of independent variables ùê± = (ùë•‚ÇÅ, ‚Ä¶, ùë•·µ£), where ùëü is the number of predictors, we assume a linear relationship between ùë¶ and ùê±: ùë¶ = ùõΩ‚ÇÄ + ùõΩ‚ÇÅùë•‚ÇÅ + ‚ãØ + ùõΩ·µ£ùë•·µ£ + ùúÄ. This equation is the regression equation. ùõΩ‚ÇÄ, ùõΩ‚ÇÅ, ‚Ä¶, ùõΩ·µ£ are the regression coefficients, and ùúÄ is the random irreducible error.\n",
    "\n",
    "</span>\n",
    "\n",
    "#####  In this regression task we will predict the percentage of marks that a student is expected to score based upon the number of hours they studied. This is a simple linear regression task as it involves just two variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Explore the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('student_scores.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Plotting:\n",
    "plot the data points on 2-D graph to eyeball our dataset and see if we can manually find any relationship between the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(x='Hours', y='Scores', style='o')\n",
    "plt.title('Hours vs Percentage')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Percentage Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print(regressor.intercept_)\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the linear regression model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task: Measure the performance of the model using the (1) root mean squared error, (2) mean absolute error and coefficient of determination $R^2$.\n",
    "\n",
    "Q: What RMSE, MSE and R-squared tell us?\n",
    "<span style=\"color:blue\"> Both RMSE and MSE tell us how much our predictions will be away from the truth on average.\n",
    "$R^2$ tells us the proportion of the variance/variability in the Y was explained by the model via the explanatory variables.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "Linear regression involving multiple variables is called \"multiple linear regression\"\n",
    "We will use multiple linear regression to predict the gas consumptions (in millions of gallons) in 48 US states based upon gas taxes (in cents), per capita income (dollars), paved highways (in miles) and the proportion of population that has a drivers license.\n",
    "We seek a model of the form:\n",
    "\n",
    "     B = A0 * X0 + A1 * X1 + A2 * X2 + A3 * X3 + A4 * X4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Explore the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dataset = pd.read_csv('petrol_consumption.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = dataset[['Petrol_tax', 'Average_income', 'Paved_Highways',\n",
    "       'Population_Driver_licence(%)']]\n",
    "y = dataset['Petrol_Consumption']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])\n",
    "coeff_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the linear regression model on test dataset\n",
    "Task: Use the predict function in the model to predict the output of the testset and call it $y\\_pred$.\n",
    "And print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Measure the performance of the model using the (1) root mean squared error, (2) mean absolute error and coefficient of determination $R^2$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred, squared=False))\n",
    "print('Root Mean Squared Error:', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Polynomial Regression, is simply a transformation for the explanatory variables to higher polynomial orders with interactive variables.\n",
    "1. Why do we need it?\n",
    "<span style=\"color:blue\"> One of the main assumptions of LR is the \"linearity\": that the relationship between X and y is linear. If this assumption doesn't hold, we need to transform the X to be close to the linearity.\n",
    "1. Which order of the polynomial should we choose? \n",
    "<span style=\"color:blue\"> Clearly, the higher the order of the polynomial, the higher the complexity of the model. This is true both computationally and conceptually because in both cases we now have a higher number of adaptable parameters. The higher the complexity of a model the more variance it can capture. Choosing the order of the model depends on the variance-bias trade-off of the model. </span>\n",
    "2. should we always pick the most complex model? \n",
    "<span style=\"color:blue\"> No: we have to strike a balance between variance and (inductive) bias: our model needs to have sufficient complexity to model the relationship between the predictors and the response, but it must have the ability to generalize to new, unseen cases. </span>\n",
    "\n",
    "#### We will create a synthetic dataset by adding some random gaussian noise to a cosinusoidal function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sinusoidal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a synthetic dataset by adding some random gaussian noise to a cosinusoidal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will build three polynomial models with degrees [1, 4, 15] and observe the effect of increasing the degree of complixity of the model on how well it suits the data.\n",
    "\n",
    "\n",
    "Task: Write a pipeline of PolynomialFeatures transformation then, LinearRegression mode to be applied on X & y.\n",
    "Then get the cross validation scores with the appropriate scoring argument with k = 10.\n",
    "\n",
    "Q: What's your observation?\n",
    "<span style=\"color:blue\">The first model with degree with degree 1 is linear, very simple, and very fast to compute but it suffers from underfitting since it does not fir the training or the testing datasets. The second model with degree 4 looks suilable for the data. The third model with degree 15 fits the training dataset too much and suffers from underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i])\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}