{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-4:\n",
    "In this lab, we will examine some classifiers and the regularization concept in the classification problem.\n",
    "Also, we will see\n",
    "\n",
    "### Objectives:\n",
    "1. Lasso and Ridge\n",
    "2. Naïve Bayes\n",
    "3. KNN\n",
    "4. Cross Validation\n",
    "\n",
    "---\n",
    "## Lasso and Ridge\n",
    "Both models are the regularized forms of the linear regression.\n",
    "Lasso with L1 regularization and Ridge with L2 regularization.\n",
    "Both act as a constraint region for the coeffeicients/weight, where they must reside in.\n",
    "\n",
    "### Issues:\n",
    "1. When to use Lasso?\n",
    "2. When to use Ridge?\n",
    "3. Since it is hard to decide the parameters influence, How we can decide which regularization? and decide the value of lambda?\n",
    "\n",
    "### Loading Boston dataset\n",
    "Housing-Prices Values in Suburbs of Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/8, random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fitting both Lasso and Ridge\n",
    "Task:\n",
    "\n",
    "Fit two models: Lasso and Ridge -with the default alpha-.\n",
    "Then print their coefficients and notice the difference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task:\n",
    "Let's try different values for alpha for Lasso regressor and plot the validation loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "alphas = [2.2, 2, 1.5, 1.3, 1.2, 1.1, 1, 0.3, 0.1]\n",
    "losses = []\n",
    "for alpha in alphas:\n",
    "    # Write (5 lines): create a Lasso regressor with the alpha value.\n",
    "    # Fit it to the training set, then get the prediction of the validation set (x_val).\n",
    "    # calculate the mean sqaured error loss, then append it to the losses array\n",
    "\n",
    "plt.plot(alphas, losses)\n",
    "plt.show()\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Mean squared error\")\n",
    "\n",
    "best_alpha = alphas[np.argmin(losses)]\n",
    "print(\"Best value of alpha:\", best_alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Measuring the loss on the testset with Lasso regressor with the best alpha."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso = Lasso(best_alpha)\n",
    "lasso.fit(x_train, y_train)\n",
    "y_pred = lasso.predict(x_test)\n",
    "print(\"MSE on testset:\", mean_squared_error(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the iris dataset\n",
    "Dataset of 3 types/classes of flowers with 4 features. Suitable for classification!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# We will show why we didn't split a validation set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naïve Bayes\n",
    "We will use the Gaussian Naïve Bayes, that deals -as a assumption- with the continous features as gaussian variables to compute their probability.\n",
    "\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}exp(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2})$$\n",
    "\n",
    "While $\\mu_y$ and $\\sigma_y^2$ are the mean and the variance of the feature $i$ for class $y$.\n",
    "\n",
    "Note: The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$.\n",
    "\n",
    "___\n",
    "What are the pros and cons of Naive bayes classifier?\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train a naive-bayes model and check the test accuracy.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gauss_nb = GaussianNB()\n",
    "gauss_nb.fit(x_train, y_train)\n",
    "y_pred = gauss_nb.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K nearest neighbour classifier\n",
    "1. What are the pros and cons of KNN?\n",
    "\n",
    "2. To decrease the variance of KNN model, should we increase or decrease the K?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Let's do the same with KNN classifier.\n",
    "\n",
    "\n",
    "Rescale the features first."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task: Let's tune the hyperparameter $n\\_neighbors$ in the KNN classifier object using the cross-validation.\n",
    "\n",
    "___\n",
    "## Cross Validation\n",
    "Cross validation comes as an alternative for the validation set splitting.\n",
    "\n",
    "Note: that's why we didn't make a validation set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "Ks = list(range(1, 20))\n",
    "cv_scores = []\n",
    "for K in Ks:\n",
    "    # Write 2 lines of code, one to compute the CV scores.\n",
    "    # And another to calculate the average.\n",
    "    scores = None\n",
    "    avg_score = None\n",
    "    # Task ends\n",
    "\n",
    "    cv_scores.append(avg_score)\n",
    "\n",
    "plt.plot(Ks, cv_scores)\n",
    "plt.show()\n",
    "print(cv_scores)\n",
    "print(\"Best K:\", Ks[np.argmax(cv_scores)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In KNN classifier, there're several hyperparamters to tune, tuning them one by one is exhaustive approach.\n",
    "Let's try a better approach called GridSearchCV.\n",
    "\n",
    "\n",
    "### GridSearchCV\n",
    "In GridSearch Cross-validation, you give different values for each hyperparamter and it will try all combinations for you.\n",
    "At the end, it will return the best combination of hyperparamters that got the best cross-validation score.\n",
    "\n",
    "Task:\n",
    "Use gridsearch to tune 3 hyperparameters:\n",
    "\n",
    "1. $n\\_neighbors$: [1, 2, . . ., 10]\n",
    "2. $weights$: ['uniform', 'distance']\n",
    "3. $metric$: ['euclidean', 'manhattan', 'chebyshev', 'cosine']\n",
    "\n",
    "Check this [link](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html) for help.\n",
    "\n",
    "Then measure the accuracy on the testset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Write 2 lines of code to run GridSearchCV with cv=7 to tune the above hyperparams.\n",
    "param_grid = None\n",
    "\n",
    "grid_search_clf = None\n",
    "\n",
    "# Task ends here!\n",
    "\n",
    "grid_search_clf.fit(x_train, y_train)\n",
    "means = grid_search_clf.cv_results_['mean_test_score']\n",
    "stds = grid_search_clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search_clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        % (mean, std * 2, params))\n",
    "print()\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(grid_search_clf.best_params_)\n",
    "\n",
    "y_pred = grid_search_clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}