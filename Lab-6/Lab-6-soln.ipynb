{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab-6 SVM:\n",
    "In this lab, we will see an examples of how to use SVM for classification tasks\n",
    "\n",
    "### Recap\n",
    "\n",
    "1. What is SVM? Is it used for classification or regression?\n",
    "<span style=\"color:blue\">Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
    "\n",
    "2. What are Support Vectors?\n",
    "<span style=\"color:blue\">Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n",
    "\n",
    "3. What is a Hyperplane?\n",
    "<span style=\"color:blue\">A hyperplane is a decision plane which separates between a set of objects having different class memberships.\n",
    "\n",
    "4. What is a Margin?\n",
    "<span style=\"color:blue\">A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n",
    "\n",
    "5. How does SVM work?\n",
    "<span style=\"color:blue\">The main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We will first consider the simple case of a classification task, in which the two classes of points are **well** separated into two classes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We will draw three lines that can separate these two classe. Depending on which line you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label! Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to consider the **margin**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We will draw around each line a **margin** of some width, up to the nearest point. The line that maximizes this margin is the one we will choose as the optimal model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support vector classifier\n",
    "Fit a simple SVC for classifing the previous data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ##### plot the svc decision function\n",
    "\n",
    "The relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero than it is assigned to the other.\n",
    "\n",
    "In order to obtain the fitted values for a given SVM model fit, we use the  .ùöçùöéùöåùöíùöúùöíùöòùöó‚éØùöèùöûùöóùöåùöùùöíùöòùöó()  method of the SVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* SVMs decision function depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in attributes support_vectors_, support_ and n_support_:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Support vectors: \", model.support_vectors_) # get support vectors\n",
    "print(\"indices of support vectors: \",model.support_) # get indices of support vectors\n",
    "print(\"number of support vectors for each class: \",model.n_support_) # get number of support vectors for each class\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Let's see how the number of samples affect the svm model and it's margin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[10, 100, 200], ax=fixed(None));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The  ùöå  argument allows us to specify the cost of a violation to the margin. When the  ùöå  argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. When the  ùöå  argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\n",
    "\n",
    "How do use find the best ùöå  argument?\n",
    "<span style=\"color:blue\"> Using cross validation\n",
    "\n",
    "We will see how to use SVM for classifing images of hand-written digits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =123)\n",
    "Cs = np.logspace(-2.3, -1.3, 10)\n",
    "cv_scores = []\n",
    "for C in Cs:\n",
    "    svc = SVC(kernel='linear', C=C)\n",
    "    scores = cross_val_score(svc, x_train, y_train,\n",
    "                             cv=7, scoring='accuracy')\n",
    "    avg_score = np.mean(scores)\n",
    "    cv_scores.append(avg_score)\n",
    "\n",
    "plt.plot(Cs, cv_scores)\n",
    "plt.show()\n",
    "print(cv_scores)\n",
    "print(Cs[np.argmax(cv_scores)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Print classification_report, confusion_matrix for the a model with the best c argument"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "svc = SVC(kernel='linear', C=0.0108)\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred = svc.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Let's use SVM for Iris dataset (Dataset of 3 types/classes of flowers with 4 features) classification and plot the decision boundaries between the classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "markers = ('s', 'x', 'o', '^', 'v')\n",
    "svc = SVC(kernel='linear', C=0.00647)\n",
    "svc.fit(X,y)\n",
    "cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "np.arange(x2_min, x2_max, 0.02))\n",
    "Z = svc.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "      plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "               alpha=0.8, c=cmap(idx),\n",
    "               marker=markers[idx], label=cl)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel SVM\n",
    "\n",
    "1. What is called a kernel trick? <span style=\"color:blue\"> It is when instead of searching hyperplane in the initial space, we are searching it in some higher-dimensional space, where projected points are linearly separable. It is called a trick because we don't actually have to project points to that space - if we know how to compute their inner products there, this is enough => saves a lot of time. </span>\n",
    "\n",
    "### Most popular kernel types\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "\n",
    "# here we assign values for constants\n",
    "n_samples = 50\n",
    "C_const = 100\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "\n",
    "# generating linearly separable data\n",
    "X_blob, Y_blob = make_blobs(n_samples=n_samples, centers=2, random_state=0, cluster_std=0.8)\n",
    "plt.subplot(\"131\")\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=Y_blob, s=50, cmap='autumn')\n",
    "plt.title(\"Blobs\")\n",
    "\n",
    "# generating moon-shaped data\n",
    "X_moon, Y_moon = make_moons(n_samples=n_samples, noise=0.1, random_state=0)\n",
    "plt.subplot(\"132\")\n",
    "plt.title(\"Moons\")\n",
    "plt.scatter(X_moon[:, 0], X_moon[:, 1], c=Y_moon, s=50, cmap='autumn')\n",
    "\n",
    "# generating concentric data\n",
    "X_circle, Y_circle = make_circles(n_samples=n_samples, factor=0.3, noise=0.1, random_state=0)\n",
    "plt.subplot(\"133\")\n",
    "plt.title(\"Circles\")\n",
    "plt.scatter(X_circle[:, 0], X_circle[:, 1], c=Y_circle, s=50, cmap='autumn')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotting function from the last lab, for your use\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=False):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=30, marker = \"x\", color=\"k\")\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trying out different kernels\n",
    "\n",
    "Task: For each dataset, try at east three kernels (linear, rbf, polynomial) with default patameters and plot the results. Analyze them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test kernels for each dataset\n",
    "from sklearn.svm import SVC\n",
    "def test_kernels(X, Y):\n",
    "    plt.figure(figsize=(20,4))\n",
    "\n",
    "    linear_model = SVC(kernel='linear')\n",
    "    linear_model.fit(X, Y)\n",
    "    plt.subplot(\"131\")\n",
    "    plt.title(\"Linear kernel\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(linear_model)\n",
    "\n",
    "    rbf_model = SVC(kernel='rbf')\n",
    "    rbf_model.fit(X, Y)\n",
    "    plt.subplot(\"132\")\n",
    "    plt.title(\"RBF kernel\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(rbf_model)\n",
    "\n",
    "    poly_model = SVC(kernel='poly', degree=3)\n",
    "    poly_model.fit(X, Y)\n",
    "    plt.subplot(\"133\")\n",
    "    plt.title(\"Polynomial kernel\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(poly_model)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_kernels(X_blob, Y_blob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_kernels(X_moon, Y_moon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_kernels(X_circle, Y_circle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring parameters effect\n",
    "Task: Try playing with C, gamma, degree parameters on the moons dataset - choose ones that you think perform best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**gamma** - determines how close should the point be to hyperplane to have any impact on it. The lower the gamma, the more elements, even those that are far enough from the dividing line, take part in the process of choosing this very line. If, however, the gamma is high, then the algorithm will \"rely\" only on those elements that are closest to the line itself.\n",
    "\n",
    "If you set the gamma level too high, then only the elements closest to the line will participate in the process of deciding on the position of the line. This will help ignore outliers in the data.\n",
    "\n",
    "\n",
    "\n",
    "![alt text](http://cs604525.vk.me/v604525210/afd4/xuJnTRcY43g.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_kernel(kernel, X, Y, C=1, gamma='scale', degree=3):\n",
    "    model = SVC(kernel=kernel, C=C, gamma=gamma, degree=degree)\n",
    "    model.fit(X, Y)\n",
    "    plt.title(kernel + \" kernel\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Manually tune parameters for the moons dataset, plotting the result\n",
    "test_kernel('rbf', X_moon, Y_moon, C=20, gamma=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}